{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import models\n",
    "\n",
    "from keras.utils import plot_model, multi_gpu_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "import utils\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 508 µs\n"
     ]
    }
   ],
   "source": [
    "classifier = '3_1_body_part_classifier_40'\n",
    "base_model = '2_1_submodel_335'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.97 ms\n"
     ]
    }
   ],
   "source": [
    "model_name = '3_2_ensemble'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    classifier = models.load_model(f'trained_models/{classifier}.h5')\n",
    "    classifier = models.Model(input=classifier.layers[-2].inputs[0], output=classifier.layers[-2].outputs[0])\n",
    "    for mid_layer in classifier.layers:\n",
    "        mid_layer.trainable = False\n",
    "\n",
    "    sub_models = []\n",
    "    for _ in range(7):\n",
    "        dn121 = DenseNet121(input_tensor=classifier.input, classes=1, weights=None, include_top=False)\n",
    "        model = models.Model(input=dn121.input, output=layers.Dense(1, activation='sigmoid')(layers.Flatten()(dn121.output)))\n",
    "        model.load_weights(f'trained_models/{base_model}.h5')\n",
    "        sub_models.append(model)\n",
    "        del dn121\n",
    "        del model\n",
    "        \n",
    "    i = 1\n",
    "    for model in sub_models:\n",
    "        for mid_layer in model.layers:\n",
    "            mid_layer.name = str(i) + '_' + mid_layer.name\n",
    "        i += 1\n",
    "    for mid_layer in classifier.layers:\n",
    "        mid_layer.name = 'c_' + mid_layer.name\n",
    "        \n",
    "    concat = layers.Concatenate(1)([model.output for model in sub_models])\n",
    "    dot = layers.Dot(axes=1)([concat, classifier.output])\n",
    "    \n",
    "    ensemble = models.Model(input=classifier.input, output=dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10 s\n"
     ]
    }
   ],
   "source": [
    "plot_model(ensemble, to_file='ensemble.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10min 23s\n"
     ]
    }
   ],
   "source": [
    "model = multi_gpu_model(ensemble, gpus=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.utils:loading data/MURA-v1.1/x_train.pkl\n",
      "INFO:utils.utils:loading data/MURA-v1.1/y_train.pkl\n",
      "INFO:utils.utils:loading data/MURA-v1.1/x_valid.pkl\n",
      "INFO:utils.utils:loading data/MURA-v1.1/y_valid.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = utils.read_mura_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 680 µs\n"
     ]
    }
   ],
   "source": [
    "size = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.98 s\n"
     ]
    }
   ],
   "source": [
    "x_train = utils.normalize_pixels(x_train)\n",
    "x_test = utils.normalize_pixels(x_test)\n",
    "x_train = x_train.reshape(x_train.shape[0], size, size, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], size, size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36808, 256, 256, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.76 ms\n"
     ]
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.28 ms\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=360,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=[1, 1.2],\n",
    "    fill_mode = 'constant',\n",
    "    cval=0,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.68 ms\n"
     ]
    }
   ],
   "source": [
    "starting_epoch = 0\n",
    "train_round = 0\n",
    "batch_size = 8\n",
    "epochs_per_eval = 10\n",
    "epochs_per_save = 30\n",
    "total_epochs = 210\n",
    "train_history = {'loss': [], 'binary_accuracy': []}\n",
    "val_per_image_history = {'accuracy': [], 'kappa': [], 'precision': [], 'recall': []}\n",
    "val_per_study_history = {'accuracy': [], 'kappa': [], 'precision': [], 'recall': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/CPU:0'):\n",
    "#     ensemble = models.load_model(f'./trained_models/{model_name}_235.h5')\n",
    "#     model = multi_gpu_model(ensemble, gpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./trained_models/{model_name}_train_history.pkl', 'rb') as pkl_file:\n",
    "#     train_history = pickle.load(pkl_file)\n",
    "# with open(f'./trained_models/{model_name}_val_per_image_history.pkl', 'rb') as pkl_file:\n",
    "#     val_per_image_history = pickle.load(pkl_file)\n",
    "# with open(f'./trained_models/{model_name}_val_per_study_history.pkl', 'rb') as pkl_file:\n",
    "#     val_per_study_history = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epochs 1-10\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "while (train_round+1) * epochs_per_eval + starting_epoch <= total_epochs:\n",
    "    print(f'executing epochs {train_round * epochs_per_eval + 1 + starting_epoch}-{(train_round+1) * epochs_per_eval + starting_epoch}')\n",
    "    \n",
    "    if train_round * epochs_per_eval + 1 + starting_epoch <= 200:\n",
    "        opt = optimizers.Adam()\n",
    "    else:\n",
    "        lr = 0.001 * (0.9 ** train_round)\n",
    "        opt = optimizers.sgd(lr=lr, momentum=0.9, nesterov=True)\n",
    "        \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=[metrics.binary_accuracy])\n",
    "    \n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                           steps_per_epoch = len(x_train) / batch_size, epochs=epochs_per_eval, verbose=2)\n",
    "    \n",
    "    train_history['loss'].extend(model.history.history['loss'])\n",
    "    train_history['binary_accuracy'].extend(model.history.history['binary_accuracy'])\n",
    "    \n",
    "    y_valid_hat = model.predict(x_test)\n",
    "    true_label = np.round(y_test)\n",
    "    pred_label = np.round(y_valid_hat)\n",
    "    evaluate = utils.MURAMetrics(true_label, pred_label)\n",
    "    \n",
    "    per_image_metrics = evaluate.report_by_image()\n",
    "    for key in per_image_metrics:\n",
    "        val_per_image_history[key].append(per_image_metrics[key])\n",
    "        print(f'Valid per image {key}: {per_image_metrics[key]}')\n",
    "    \n",
    "    per_study_metrics = evaluate.report_by_study()\n",
    "    for key in per_image_metrics:\n",
    "        val_per_study_history[key].append(per_study_metrics[key])\n",
    "        print(f'Valid per study {key}: {per_study_metrics[key]}')\n",
    "    \n",
    "    cur_epoch = (train_round+1) * epochs_per_eval\n",
    "    if cur_epoch % epochs_per_save == 0:\n",
    "        print(f'Saving Model to trained_models/{model_name}_{cur_epoch + starting_epoch}.h5...')\n",
    "        model.save(f'./trained_models/{model_name}_{cur_epoch + starting_epoch}.h5')\n",
    "    \n",
    "    print('Saving evaluation metrics history...')\n",
    "    with open(f'./trained_models/{model_name}_train_history.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(train_history, pkl_file)\n",
    "    with open(f'./trained_models/{model_name}_val_per_image_history.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(val_per_image_history, pkl_file)\n",
    "    with open(f'./trained_models/{model_name}_val_per_study_history.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(val_per_study_history, pkl_file)\n",
    "    \n",
    "    train_round += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plots for epochs 145-365\n",
    "# 145-235 used adam with default parameters\n",
    "# 236-345 used lr = 0.001 * (0.9 ** train_round)\n",
    "# 346-365 used lr = 0.0001 * (0.9 ** train_round) (pretty much levelled off, use 335 as result should be good enough)\n",
    "train_metrics = ['loss', 'binary_accuracy']\n",
    "validation_metrics = ['accuracy', 'kappa', 'precision', 'recall']\n",
    "for metric in train_metrics:\n",
    "    plt.plot(history[metric])\n",
    "plt.title('Training set metrics per epoch')\n",
    "plt.legend(train_metrics)\n",
    "plt.show() \n",
    "\n",
    "for metric in validation_metrics:\n",
    "    plt.plot(history[metric])\n",
    "plt.title(f'Validation set metrics per {epochs_per_eval}')\n",
    "plt.legend(validation_metrics)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
