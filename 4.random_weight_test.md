This is a quick proof-of-concept test of Saxe 2011 paper [On Random Weights and Unsupervised Feature Learning](http://www.icml-2011.org/papers/551_icmlpaper.pdf)

Saxe found that random CNN weights are usually almost as good as pre-trained weights, and we can separate the predictive power contributed by model structure from the power contributed by training optimization. Hence by using random initialized weights and only train the top layer, it allows for rapid artecture comparison.

First observation: Need to use weighted loss - otherwise network just learns to predict zero for all cases. 
(Note: this is also done before I started tuning learning rate - might get a different result if cyclical learn rate between 1e-6 and 1e-5 is used)

From comparison between DenseNet and MobileNet the result does seem to make sense - DenseNet reached study-wise Kappa = 0.3 in 30 epochs, while MobileNet only reached 0.2 after 80 epochs. 

(Will do more experiment on this)