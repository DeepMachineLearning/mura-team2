# Ensemble with Shared Base Layer


### [5.0: Training the shared base layers](5_0_base_layers.ipynb)
Trained DenseNet169 on all images - the first three dense blocks will be used as shared base layers for the ensemble.
- Training used cyclical learn rate on Adam, with the 'triangular2' method (decreaes range by half every specified number of iterations), and the starting learn range is [5e-6, 5e-5]. Weights are collapsed pre-trained weights from ImageNet
    - Exceeded previous best performance on random weights with default 0.01 learn rate in 30 epochs
    - Started to overfit after 30 epochs. Best performance Kappa=0.635


### [5.1: Training ensemble with shared base layers](5_1_shared_net.ipynb)
Used `keras_applications` directly to build this custom ensemble network. Network architecture is plotted at [5_1_ensemble.png](5_1_ensemble.png).
- Seem to be able to improve results slightly
- training is extermely slow due to the size of model (~0.5 hrs per epoch)
- best performance Kappa=0.645