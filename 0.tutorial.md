# Tutorials


### [0.0: Intro to Keras with MNIST](0_0_keras_tutorial_with_mnist.ipynb)
A tutorial for Keras on MNIST. Taken from https://elitedatascience.com/keras-tutorial-deep-learning-in-python

### [0.1: MURA data pre-processing](0_1_read_pickle_mura_data.ipynb)
A wrapper for importing MURA images as grayscale, padding them to a square, reshaping and saving the resulting numpy array into a pickle file

### [0.2: Keras tutorial applied on MURA](0_2_keras_tutorial_with_mura.ipynb)
The same tutorial as 0.0 on MURA dataset

### [0.3: Some digging into Cohen's Kappa](0_3_notes_on_cohens_kappa.ipynb)
Some notes on the mathematical properties of Cohen's Kappa, which is the official evaluation metric
Discussion:
- Interestingly, assuming accuracy is fixed, Cohen's Kappa is lower when the number of false positives and false negatives are balanced. It makes sense as it decreases the expected probability of getting the same label assuming each classifier classifies randomly.
- Maybe we can increase Kappa by moving all the predictions near 0.5 to one side? Since the predictions near 0.5 should have a half-half chance of being right or wrong.

### [0.4: Initial Data Exploration](0_4_data_explore.ipynb)
Some initial data exploration
Discussion:
- given number of images, the target class is very unbalanced. The distribution roughly carries into validation data, but we can't be sure that it'll be the same in test data.
    - Need to be extra careful when training network that takes all available images
- 

More exploration in [0.6: Model Diagnostics](0_6_diagnostics.ipynb)